<!DOCTYPE html>
<html >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <base target="_self"> 

  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" />

  <link rel="stylesheet" href="{{ url_for('static', filename='css/styles.css') }}" /> 
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
</head>
<body >

<!--
NAV SECTION
-->    

    <div class="">
        <div class="container remove_padding">
            <nav class="navbar navbar-fixed-top navbar-expand-md bg-purple navbar-dark">
              <a class="navbar-brand align-self-start" href="index.html">TLDW; Too Long Didn't Watch</a>
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item active">
                        <a class="nav-link js-scroll-trigger" href="#tryit">Try It</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#model">Models/Architecture</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#data">Data</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#team">The Team</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#about">About</a>
                    </li>
                </ul>
            </nav>
        </div>    
    </div>


<!-- CONTENT SECTION -->

    <div class="container bg-white text-center text-black">
                 <!--  <div class="container" style="background: purple;">. -->
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <h2>Models & Architecture</h2>
                    <hr>
                    <p class="lead text-left">There were many steps and models explored in the creation of Too Long Didn't Watch, along with interesting challenges. As a team, we generally expected product review videos to be organized and relatively easy to "process" through NLP libraries. However, two areas  required significant attention.</p>
                    <p class="lead text-left">First, many videos are "auto-captioned" which means YouTube automatically adds captions but no punctuation. Video posters can add their own captions and include punctuation, but captioning a 20-min video can be tedious. The lack of punctuation provided a challenge when trying to identify sentences and associated product features with descriptors. Second, many product review videos are conversational in nature, complete with filler words (um's/uh's/you know/like) and frequent jumping back and forth between topics and product features.</p>
                    <p class="lead text-left">During the early stages of our project, one of the first tasks that needed to be explored was how to extract the audio from a review video and convert it to text.</p>
                    <p class="lead text-left">Several methods were built including a function that extracted the audio track and then converted the audio track to text. YouTube also provides a library to extract captions from videos. This video caption extractor required only one step (vs 5 when first extracting the audio track), so we opted to use YouTube's caption extractor.</p>
                    <p class="lead text-left">After identifying the method to extract the video voice-over into text, we needed to figure out a way to extract keywords and sentences.</p>
                </div>
            </div>
        </div>


        <div class="container text-center">
              <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h3>Keyword Extractor</h3>
                    </div>
              </div>
        </div>

        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <p class="lead text-left">We defined keywords as product features with associated decriptors and experimented with many models.</p> 
                </div>
            </div>    
        </div>


    <div class="container text-center">
          <div class="row">
                <div class="col-lg-12 mx-auto">

                  <img class="img-responsive" src="{{ url_for('static', filename='images/KW_Extractor_Models.png') }}"> 
                  <hr>
                </div>
          </div>
    </div>


        <div class="container">
                 <!--  <div class="container" style="background: purple;">. -->
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <p class="lead text-left"><p class="lead text-left">An example of keyword output for each model is shown below. We noticed that the majority of keywords are usually in sentences that have a very positive sentiment or very negative sentiment. Rarely do we find keywords in sentences that have a neutral sentiment, or just a slightly positive/negative sentiment. So we implemented a sentiment filter to aid in identifying keywords. The sentiment filter "removes" the less useful sentences prior to keyword extraction.</p>
                </div>
            </div>    
        </div>

      <div class="container text-center">
          <div class="row">
                <div class="col-lg-12 mx-auto">
                  <h5>Keyword Extractor: Example Output</h5>
                  <img class="img-responsive" src="{{ url_for('static', filename='images/KW_output.png') }}"> 

                </div>
          </div>
      </div>

      <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <p class="lead text-left">To quantitatively score our models' performance, we developed our own formula. This required us to manually read video text output and highlight relevant keywords (because we did not have training data). The process helped us visually identify what should be included in the output. During the manual review process we also noticed that keywords tend to be in sentences that have fairly strong negative or positive sentiment. Hence our application of a sentiment filter.</p> 
                    <p class="lead text-left">Rationale for the formula below is our desire for balance between the number of keywords that are correct and the number of keywords actually generated. The likelihood of getting more correct keywords is partially a function of simply generating more keywords overall. But this results in also getting more incorrect keywords which defeats the purpose of the model. Our end goal is to deliver as many correct keywords with the least amount of overall keywords generated. Our score calculation:</p>
                </div>
            </div>    
        </div>

    <div class="container text-center bg-info">
          <div class="row">
              <div class="col-lg-8 mx-auto">
                <img class="img-responsive" src="{{ url_for('static', filename='images/score.png') }}"> 

              </div>
          </div>
    </div>
    
        <div class="container text-center"
            <div class="row">
                <div class="col-lg-10 mx-auto">
                
                    <p class="lead text-left">Because the sentiment filter removes neutral sentences, as well as slightly positive/negatie sentences, we tested scores with and without the sentiment filter. As seen in the table below, the models performed better when the sentiment filter was applied.</p>
                    <br>
                    <img class="img-responsive" src="{{ url_for('static', filename='images/sentiment_score.png') }}">
                </div>
            </div>
        </div>

      <div class="container text-center"
            <div class="row">
                <div class="col-lg-10 mx-auto">
                
                    <p class="lead text-left">Below is a snapshot of each model's performance when applying our custom scoring formula. Our end solution uses Singlerank (which is the green line).</p>
                    <br>
                    <h5>Keyword Extractor: Model Performance</h5>
                    <div class="col-lg-8 mx-auto">
                      <img class="img-responsive" src="{{ url_for('static', filename='images/kw_extract_performance.png') }}">
                    </div>  
                    <hr>
                </div>
            </div>
      </div>


        <div class="container text-center">
              <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h3>Statement/Phrase Extractor</h3>
                    </div>
              </div>
        </div>            


      <div class="container text-center">
          <div class="row">
                <div class="col-lg-10 mx-auto">
                  <p class="lead text-left">In addition to providing sentiment-colored keywords as output for our user (green for positive, red for negative), we also thought it would be useful to show the entire narrative from the video. In order to do this, we needed to create a statement extractor. Again, this is because many videos do not come with customized captions that include punctuation.</p>
                  <br>
                  <img class="img-responsive" src="{{ url_for('static', filename='images/statement_extractor.png') }}"> 
                  <br>
                  <br>
                  <p class="lead text-left">To begin, we manually review the text output and identify is/are statements that are useful for the user. Our code will then implement the following:</p>
                  <div class="col-lg-10 mx-auto text-left">
                    <li>Preprocess text to convert “it’s” to “it is”, and “they’re” to “they are”</li>
                    <li>Find “is” or “are” tokens and use Spacy to extract the subtree associated with that token</li>
                    <li>Review all words in the subtree to the left of the token and use POS (noun, adj, etc), TAG (VBN, UH, etc), and DEP (nsubj, expl, etc) to look for the subject of the statement</li>
                    <li>Look words in the subtree to the right of the token and identify the most relevant description using the same method but a different set of rules</li>
                  </div>
                  <br>
                  <p class="lead text-left">The above methods work with text that does not contain punctuation, but the rules must be stricter to extract coherent statements. The stricter rules also result in fewer sentences being extracted.</p>
                  <p class="lead text-left">We also created a formula to score sentence extraction. Text output with punctuation performs well. But if text output does not include punctuation, stricter rules are required because there are more "missed guesses" which lowers the score. </p>
                  <p class="lead text-left">One interesting note tying back to our initial observations regarding "conversational talk", filler words, and unorganized flow: if a product review video has a lot of useless is/are statements, our sentence scores may be negatively impacted because of the numerous "useless" sentence guesses in the output.</p>
                  <p class="lead text-left">In the table below, it's important to note whether punctuation is included in the output automatically or not as videos including punctuation in the captions were much more easy to identify.</p>
                </div>
          </div>    
    </div>

      <div class="container text-center">
              <div class="row">
                    <div class="col-lg-12 mx-auto">
                        <img class="img-responsive" src="{{ url_for('static', filename='images/statement_formula.png') }}"> 
                        <br>
                    </div>
              </div>
        </div>

          <div class="container"
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <p class="lead text-left">Per the above table, the scores for accuracy of sentence extraction <b>when captions include punctuation</b> is significantly better than when the video captions do not contain punctutation. The number of Missed Guesses ranged between <b>3%-6% for sentences with punctutation</b> versus <b>24%-39% for sentences without punctuation.</b></p>
                    <hr>
                </div>
            </div>
        </div>


        <div class="container text-center">
              <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h3>Summarizer</h3>
                    </div>
              </div>
        </div>     


        <div class="container"
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <p class="lead text-left">In order to create the Summarizer element in the output, the Amazon Reviews data was used. (For detailed information on the data, please reference the "Data" page on this site.)</p>
                    <p class="lead text-left">Before beginning supervised learning, the data was further cleaned. Reviews with the following were removed:</p>
                    <div class="col-lg-10 mx-auto text-left">
                      <li>"Summary" field of the review (filled in by reviewer) included the word 'star' or only had two words</li>
                      <li>Body of the review contained three or fewer words</li>
                      <li>Body of review was an HTML tag</li>
                    </div>
                    <p class="lead text-left">Next, the data was used to train an Encoder-Decoder (Seq2Seq) model with attention. Four variations of the model were run to determine the ideal configuration.</p>
                    <div class="col-lg-10 mx-auto text-left">
                      <li>1. Encoder: single LSTM</li>
                      <li>   Decoder: single LSTM</li>
                      <li>2. Encoder: 3-layer LSTM</li>
                      <li>   Decoder: single LSTM</li>
                      <li>3. Encoder: single bi-LSTM</li>
                      <li>   Decoder: single LSTM</li>
                      <li>4. Encoder: 3-layer bi-directional LSTM's</li>
                      <li>   Decoder: single LSTM</li>
                    </div> 
                    <br>
                    <p class="lead text-left">We applied the Rouge score to identify the best variation. (Rouge is commonly used to evaluate NLP summarization. Rouge metrics compare an machine learning generated summary output to a human-produced piece of text. In this case the human-produced text is the Amazon Reviews data.)</p> 
                    <p class="lead text-left">Because of the poor quality of the "Summary" field in the Amazon Review data, the Encoder-Decoder models didn't perform as well as expected. For the existing tool, we opted to use the 3-layer BiLSTM. But further improvements are necessary.</p> 
                  </div>
                </div>
            </div>
        </div>

        <div class="container text-center">
              <div class="row">
                    <div class="col-lg-12 mx-auto">
                        <img class="img-responsive" src="{{ url_for('static', filename='images/LSTM.png') }}"> 
                        <br>
                    </div>
              </div>
        </div>


<<!-- FOOTER -->
    <br>
    <br>
    <div class="row text-center justify-content-center">
              
        <div class="col-lg-6">

            <footer class="footer px-5">
            <p>© VideoReview 2019</p>
            </footer>
        </div>   
    </div>

  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>

  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>

  <script src="js/scripts.js"></script>
</body>
</html>


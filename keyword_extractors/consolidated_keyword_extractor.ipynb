{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby, chain\n",
    "from collections import Counter, defaultdict\n",
    "import boto3\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import pke\n",
    "\n",
    "#!python -m nltk.downloader stopwords\n",
    "#!python -m nltk.downloader universal_tagset\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class SentimentFilter(object):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sent_df = None\n",
    "\n",
    "    def getSentiment(self):\n",
    "        \"\"\"Get sentiment for each sentence in the text\"\"\"\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        doc = nlp(self.text)\n",
    "        sentence_tokens = [sents.text for sents in doc.sents]\n",
    "        sent_list = []\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "        neu_list = []\n",
    "        mix_list = []\n",
    "\n",
    "        def sentenceSentiment(text):\n",
    "            comprehend = boto3.client(service_name='comprehend', region_name='us-west-2')\n",
    "            sentiment_json = comprehend.detect_sentiment(Text=text, LanguageCode='en')\n",
    "            sent = sentiment_json['Sentiment']\n",
    "            sent_pos = sentiment_json['SentimentScore']['Positive']\n",
    "            sent_neg = sentiment_json['SentimentScore']['Negative']\n",
    "            sent_neu = sentiment_json['SentimentScore']['Neutral']\n",
    "            sent_mix = sentiment_json['SentimentScore']['Mixed']\n",
    "\n",
    "            return sent, sent_pos, sent_neg, sent_neu, sent_mix\n",
    "\n",
    "        for s in sentence_tokens:\n",
    "            a, b, c, d, e = sentenceSentiment(s)\n",
    "            sent_list.append(a)\n",
    "            pos_list.append(b)\n",
    "            neg_list.append(c)\n",
    "            neu_list.append(d)\n",
    "            mix_list.append(e)\n",
    "\n",
    "        df = pd.DataFrame({'sentence': sentence_tokens, 'sentiment': sent_list, 'pos_prob': pos_list, \n",
    "                           'neg_prob': neg_list, 'neutral_prob':neu_list, 'mixed_prob': mix_list })\n",
    "        df = df.round(2)\n",
    "        self.sent_df = df\n",
    "\n",
    "    def filterNeutrals(self):\n",
    "        \"\"\"\n",
    "        Filters out all neutral sentences.\n",
    "        \"\"\"\n",
    "        self.sent_df = self.sent_df[self.sent_df['sentiment'] != 'NEUTRAL']\n",
    "        self.sent_df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        \n",
    "    def filterPositives(self, threshold=0):\n",
    "        \"\"\"\n",
    "        Filters out positive sentiment sentences with score below the threshold.\n",
    "        \"\"\"\n",
    "        self.sent_df = self.sent_df[~((self.sent_df['sentiment'] == 'POSITIVE') &\n",
    "                                      (self.sent_df['pos_prob'] < threshold))]\n",
    "        self.sent_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    def filterNegatives(self, threshold=0):\n",
    "        self.sent_df = self.sent_df[~((self.sent_df['sentiment'] == 'NEGATIVE') &\n",
    "                                      (self.sent_df['neg_prob'] < threshold))]\n",
    "        self.sent_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    def filterMixed(self, threshold=0):\n",
    "        \"\"\"\n",
    "        Filters out mixed sentiment sentences with score below the threshold.\n",
    "        \"\"\"\n",
    "        self.sent_df = self.sent_df[~((self.sent_df['sentiment'] == 'MIXED') &\n",
    "                                      (self.sent_df['mix_prob'] < threshold))]\n",
    "        self.sent_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    def getDataFrame(self):\n",
    "        \"\"\"\n",
    "        Get the sentiment dataframe.\n",
    "        \"\"\"\n",
    "        return self.sent_df\n",
    "    \n",
    "    def getFilteredTokens(self):\n",
    "        \"\"\"\n",
    "        Get the sentence tokens from the text.\n",
    "        \"\"\"\n",
    "        self.filtered_tokens = list(self.sent_df['sentence'])\n",
    "        return self.filtered_tokens\n",
    "    \n",
    "    def getFilteredText(self):\n",
    "        \"\"\"\n",
    "        Get the text that has had sentences filtered out.\n",
    "        \"\"\"\n",
    "        self.filtered_text = TreebankWordDetokenizer().detokenize(list(self.sent_df['sentence']))\n",
    "        return self.filtered_text\n",
    "\n",
    "\n",
    "class myRake(object):\n",
    "    \"\"\"Rapid Automatic Keyword Extraction Algorithm customized for \n",
    "    key-word extraction on video text w/ or w/o punctuation.\n",
    "    \n",
    "    RAKE algorithm based off of implementation from rake-nltk by Vishwas B Sharma\n",
    "    https://github.com/csurfer/rake-nltk with changes to suit personal needs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stopwords = None, punctuations = None, num_words = 100000,\n",
    "                 use_POS = True, known_words = []):\n",
    "        # Initialize the stopwords and punctuations used to break text into phrases\n",
    "        self.stopwords = stopwords\n",
    "        self.punctuations = punctuations\n",
    "        if self.stopwords == None:\n",
    "            self.stopwords =  nltk.corpus.stopwords.words('english')\n",
    "        if self.punctuations == None:\n",
    "            self.punctuations = list('!\"#$%&\\'()*+,./:;<=>?@[-\\\\]^_`{|}~♪')\n",
    "        # This is the set of words that determines breaks between phrases\n",
    "        self.phrase_breaks = set(self.stopwords + self.punctuations)\n",
    "        \n",
    "        # This variable determines how many words long our key-words can be\n",
    "        self.num_words = num_words\n",
    "        \n",
    "        # This variable lets us know if we want to use regular stopwords, or incorporate POS\n",
    "        self.use_POS = use_POS\n",
    "        # This variable stores a list of words that we want to have more impact in terms of score\n",
    "        self.known_words = known_words\n",
    "        \n",
    "        # Variables to calcuate RAKE score\n",
    "        self.frequencies = None\n",
    "        self.degrees = None\n",
    "        self.key_words = None\n",
    "        \n",
    "    def extract_keywords(self, text):\n",
    "        # Situation where text contains sentences/punctuation\n",
    "        if \", \" in text:\n",
    "            text_list = nltk.tokenize.sent_tokenize(text)\n",
    "            phrase_tuples = self.key_word_candidates(text_list)\n",
    "            self.RAKE_score(phrase_tuples)\n",
    "            \n",
    "        # Situation where text does not contain sentences/punctuation\n",
    "        else:\n",
    "            text_list = nltk.tokenize.sent_tokenize(text)\n",
    "            phrase_tuples = self.key_word_candidates(text_list)\n",
    "            self.RAKE_score(phrase_tuples)\n",
    "            # TO DO: add some sort of method to split the text up into multiple sentences\n",
    "            # Convert string to list of words. After x number of words, if the word and next word do not fall in\n",
    "            # ['ADJ','DET','NOUN','NUM','PART','PROPN'] category, then add a . Then convert back to string\n",
    "        \n",
    "    def spacy_POS_phrase_breaks(self, text):\n",
    "        \"\"\"\n",
    "        Inputs a string of text, find the Part of Speech for each word and add words that are not\n",
    "        ['ADJ','DET','NOUN','NUM','PART','PROPN'] into a set of phrase break words to ignore.\n",
    "        \"\"\"\n",
    "        # These are POS tags that we want in our keywords.\n",
    "        # Try removing ADJ, DET \n",
    "        POS_we_want = ['ADJ','DET','NOUN','NUM','PART','PROPN']\n",
    "        # Initialize the set with our existing phrase breaks\n",
    "        temp_phrase_breaks = self.phrase_breaks\n",
    "        \n",
    "        # Use spacy to tag POS and then only keep words with the POS that we want\n",
    "        doc = spacy_nlp(text)\n",
    "        for token in doc:\n",
    "            if token.pos_ not in POS_we_want:\n",
    "                temp_phrase_breaks.add(token.text.lower())\n",
    "        return temp_phrase_breaks\n",
    "                \n",
    "        \n",
    "    def key_word_candidates(self, text_list):\n",
    "        \"\"\"\n",
    "        Input a list of text segments and generates a set of possible key-word candidates.\n",
    "        \"\"\"\n",
    "        candidates = set()\n",
    "        for text in text_list:\n",
    "            # Extract all words and punctuation from text into a list\n",
    "            words = [word.lower() for word in nltk.wordpunct_tokenize(text)]\n",
    "            \n",
    "            if self.use_POS:\n",
    "                # Create a temporary set of break words based on the Part of Speech\n",
    "                temp_phrase_breaks = self.spacy_POS_phrase_breaks(text)\n",
    "                # group words together using phrase breaks and a separator \n",
    "                phrase_groups = groupby(words, lambda word: word not in temp_phrase_breaks)\n",
    "                \n",
    "            else:\n",
    "                # if we don't want to use POS, just use the stopwords + punct to break phrases\n",
    "                phrase_groups = groupby(words, lambda word: word not in self.phrase_breaks)\n",
    "                \n",
    "            # Pull out the groups of words that do not include any of the phrase breaks   \n",
    "            phrase_tuples = [tuple(group[1]) for group in phrase_groups if group[0] == True]\n",
    "            # Add these groups to the output set\n",
    "            candidates.update(phrase_tuples)\n",
    "        # make sure the number of words in each of the tuples does not go over our limit\n",
    "        return set(filter(lambda x: len(x) <= self.num_words, candidates))\n",
    "        \n",
    "    def RAKE_score(self, phrase_tuples):\n",
    "        \"\"\"\n",
    "        Frequency part: chain up the phrase tuples and use the counter to tally up how often each word occurs.\n",
    "                        Saves a dictionary of word:count pairs in self.frequencies\n",
    "        Degree part: create a default dict to keep track of how many words each word co-occurs with in \n",
    "                     the phrase tuples. There is another way that keeps track of a co-occurence graph which\n",
    "                     might be useful but I didn't implement for the sake of simplicity.\n",
    "        Scoring part: Calculate the RAKE score for each phrase. The RAKE score for each  word is degree/frequency\n",
    "                      and the RAKE score for each phrase is the sum of each word's RAKE score.\n",
    "        \"\"\"\n",
    "        # Frequency part\n",
    "        self.frequencies = Counter(chain.from_iterable(phrase_tuples))\n",
    "        \n",
    "        # Degree part\n",
    "        self.degrees = defaultdict(int)\n",
    "        for phrase in phrase_tuples:\n",
    "            for word in phrase:\n",
    "                self.degrees[word] += len(phrase)\n",
    "        \n",
    "        # Scoring part\n",
    "        self.key_words = defaultdict(float)\n",
    "        phrases = list()\n",
    "        scores = list()\n",
    "        for phrase in phrase_tuples:\n",
    "            score = 0.0\n",
    "            for word in phrase:\n",
    "                score += float(self.degrees[word])/float(self.frequencies[word])\n",
    "                # This is to give words that we know should be keywords a boost in score\n",
    "                if word in self.known_words:\n",
    "                    score += 10\n",
    "            phrases.append(\" \".join(phrase))\n",
    "            scores.append(score)\n",
    "        phrases = np.array(phrases)\n",
    "        scores = np.array(scores)\n",
    "        # Store the phrase:score pairs in descending order into self.key_words\n",
    "        for i in np.argsort(scores)[::-1]:\n",
    "            self.key_words[phrases[i]] = scores[i]\n",
    "    \n",
    "    def get_key_words(self, n = None):\n",
    "        \"\"\"\n",
    "        get command to return a list of keywords ordered by their RAKE score\n",
    "        n is the number of words to output\n",
    "        \"\"\"\n",
    "        if n == None:\n",
    "            return list(self.key_words.keys())\n",
    "        else:\n",
    "            return list(self.key_words.keys())[:n]\n",
    "    \n",
    "    def get_key_words_scores(self):\n",
    "        \"\"\"\n",
    "        get command to return a list of keywords and their RAKE scores\n",
    "        \"\"\"\n",
    "        return [(key,self.key_words[key]) for key in self.key_words]\n",
    "\n",
    "\n",
    "class KeyWordExtractor(object):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sent_df = None\n",
    "        self.kw_tfidf = []\n",
    "        self.kw_kpminer = []\n",
    "        self.kw_yake = []\n",
    "        self.kw_rake = []\n",
    "        self.kw_textrank = []\n",
    "        self.kw_singlerank = []\n",
    "        self.kw_topicrank = []\n",
    "        self.kw_tprank = []\n",
    "        self.kw_positionrank = []\n",
    "        self.kw_mprank = []\n",
    "        \n",
    "    def tfidf(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.TfIdf()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_tfidf = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using KP-Miner')       \n",
    "                  \n",
    "    def kpMiner(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.KPMiner()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_kpminer = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using KP-Miner')\n",
    "        \n",
    "    def yake(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.YAKE()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_yake = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using YAKE')\n",
    "    \n",
    "    def rake(self, n=20):\n",
    "        try:\n",
    "            extractor = myRake(use_POS=True)\n",
    "            extractor.extract_keywords(self.text)\n",
    "            self.kw_rake = extractor.get_key_words_scores()[:n]\n",
    "        except:\n",
    "            print('Failed to extract keywords using KP-Miner')\n",
    "\n",
    "    def textRank(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.TextRank()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_textrank = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using TextRank')\n",
    "                  \n",
    "    def singleRank(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.SingleRank()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_singlerank = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using SingleRank')\n",
    "       \n",
    "    def topicRank(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.TopicRank()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_topicrank = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using TopicRank')\n",
    "    \n",
    "    def topicalPageRank(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.TopicalPageRank()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_tprank = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using Topical PageRank')\n",
    "        \n",
    "    def positionRank(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.PositionRank()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_positionrank = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to exract keywords using PositionRank')\n",
    "\n",
    "    def multiPartiteRank(self, n=20):\n",
    "        try:\n",
    "            extractor = pke.unsupervised.MultipartiteRank()\n",
    "            extractor.load_document(self.text, language='en')\n",
    "            extractor.candidate_selection()\n",
    "            extractor.candidate_weighting()\n",
    "            self.kw_mprank = extractor.get_n_best(n=n)\n",
    "        except:\n",
    "            print('Failed to extract keywords using Multi-Partite Rank')\n",
    "\n",
    "\n",
    "    def allExtractors(self, n=20, pos=None, window=10, normalized=False):\n",
    "        self.tfidf(n=n)\n",
    "        self.kpMiner(n=n)\n",
    "        self.yake(n=n)\n",
    "        self.rake(n=n)\n",
    "        self.textRank(n=n)\n",
    "        self.singleRank(n=n)\n",
    "        self.topicRank(n=n)\n",
    "        self.topicalPageRank(n=n)\n",
    "        self.positionRank(n=n)\n",
    "        self.multiPartiteRank(n=n)\n",
    "        \n",
    "    def getAllKeyWords(self, include_score=False, sort=False):\n",
    "        \"\"\"\n",
    "        Get all keywords for all models\n",
    "        \"\"\"\n",
    "        if include_score == True:\n",
    "            all_keywords = {'tfidf_keywords': self.kw_tfidf, \n",
    "                            'kpminer_keywords': self.kw_kpminer, \n",
    "                            'yake_keywords': self.kw_yake,\n",
    "                            'rake_keywords': self.kw_rake, \n",
    "                            'textrank_keywords': self.kw_textrank,\n",
    "                            'singlerank_keywords':self.kw_singlerank,\n",
    "                            'topicrank_keywords': self.kw_topicrank, \n",
    "                            'topicalpagerank_keywords': self.kw_tprank, \n",
    "                            'position_keywords': self.kw_positionrank,\n",
    "                            'multipartiterank_keywords': self.kw_mprank}\n",
    "        else:\n",
    "            if sort == False:\n",
    "                all_keywords = {'tfidf_keywords': [i[0] for i in self.kw_tfidf],\n",
    "                                'kpminer_keywords': [i[0] for i in self.kw_kpminer], \n",
    "                                'yake_keywords': [i[0] for i in self.kw_yake],\n",
    "                                'rake_keywords': [i[0] for i in self.kw_rake],\n",
    "                                'textrank_keywords': [i[0] for i in self.kw_textrank],\n",
    "                                'singlerank_keywords': [i[0] for i in self.kw_singlerank],\n",
    "                                'topicrank_keywords': [i[0] for i in self.kw_topicrank], \n",
    "                                'topicalpagerank_keywords': [i[0] for i in self.kw_tprank],\n",
    "                                'position_keywords': [i[0] for i in self.kw_positionrank],\n",
    "                                'multipartiterank_keywords': [i[0] for i in self.kw_mprank]}\n",
    "            else:\n",
    "                all_keywords = {'tfidf_keywords': sorted([i[0] for i in self.kw_tfidf]),\n",
    "                                'kpminer_keywords': sorted([i[0] for i in self.kw_kpminer]), \n",
    "                                'yake_keywords': sorted([i[0] for i in self.kw_yake]),\n",
    "                                'rake_keywords': sorted([i[0] for i in self.kw_rake]),\n",
    "                                'textrank_keywords': sorted([i[0] for i in self.kw_textrank]),\n",
    "                                'singlerank_keywords': sorted([i[0] for i in self.kw_singlerank]),\n",
    "                                'topicrank_keywords': sorted([i[0] for i in self.kw_topicrank]), \n",
    "                                'topicalpagerank_keywords': sorted([i[0] for i in self.kw_tprank]),\n",
    "                                'position_keywords': sorted([i[0] for i in self.kw_positionrank]),\n",
    "                                'multipartiterank_keywords': sorted([i[0] for i in self.kw_mprank])}\n",
    "\n",
    "        return all_keywords\n",
    "\n",
    "class keywordFilter(object):\n",
    "    \n",
    "    def __init__(self, keywords, sentiment_df):\n",
    "        self.keywords = keywords\n",
    "        self.sentiment_df = sentiment_df\n",
    "\n",
    "        # Get the sentences tied to all the keywords\n",
    "        kw_dict = {}\n",
    "        for kw in self.keywords:\n",
    "            kw_dict[kw] = list(self.sentiment_df[self.sentiment_df['sentence']\n",
    "                                                     .str.lower()\n",
    "                                                     .str.contains(kw)]['sentence'])\n",
    "\n",
    "        # Create dataframe out of the dictionary\n",
    "        keyword_df = pd.DataFrame.from_dict(kw_dict, orient='index')\n",
    "        keyword_df = keyword_df.stack().to_frame('sentence').reset_index()\n",
    "        keyword_df.drop('level_1', axis=1, inplace=True)\n",
    "        keyword_df.columns = ['keyword', 'sentence']\n",
    "\n",
    "        # join in the sentiment for each sentence\n",
    "        keyword_df = keyword_df.set_index('sentence').join(sentiment_df.set_index('sentence'))\n",
    "        keyword_df.reset_index(inplace=True)\n",
    "\n",
    "        # filter down to the necessary columns\n",
    "        keyword_df = keyword_df[['keyword', 'sentence', 'sentiment']]\n",
    "        self.keyword_df = keyword_df\n",
    "    \n",
    "    def sentenceCountFilter(self, n=1):\n",
    "        count_df = self.keyword_df['keyword'].value_counts().to_frame('sentence_count')\n",
    "        count_df = count_df[count_df['sentence_count'] <= n] # Put variable here for filtering\n",
    "        count_df.reset_index(inplace=True)\n",
    "        self.keyword_df = self.keyword_df[self.keyword_df['keyword'].isin(count_df['index'])].copy()\n",
    "        self.keyword_df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def duplicateFilter(self):\n",
    "        self.keyword_df['keyword_num'] = self.keyword_df.groupby('sentence')['keyword']\\\n",
    "                                                        .expanding()\\\n",
    "                                                        .count()\\\n",
    "                                                        .to_frame()\\\n",
    "                                                        .reset_index()['keyword']\n",
    "\n",
    "        self.keyword_df = self.keyword_df[self.keyword_df['keyword_num'] == 1].copy()\n",
    "        self.keyword_df.drop('keyword_num', axis=1, inplace=True)\n",
    "        self.keyword_df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def getKeywordDataFrame(self):\n",
    "        return self.keyword_df\n",
    "\n",
    "def outputKeywords(text):\n",
    "\n",
    "    # Filter out neutral sentences and sentences that are only slightly positive/negative\n",
    "    filtered_text = SentimentFilter(text)\n",
    "    filtered_text.getSentiment()\n",
    "    filtered_text.filterPositives(0.75)\n",
    "    filtered_text.filterNegatives(0.75)\n",
    "    filtered_text.filterNeutrals()\n",
    "    sentiment_df = filtered_text.getDataFrame()\n",
    "\n",
    "    # Extract keywords\n",
    "    model_keywords = KeyWordExtractor(filtered_text.getFilteredText())\n",
    "    model_keywords.singleRank(n=20)\n",
    "    model_keywords_df = pd.DataFrame(model_keywords.kw_singlerank,columns=['keyword', 'score'])\n",
    "\n",
    "    # Filter out duplicated keywords and keywords that appear in multiple sentences\n",
    "    dupe_filter = keywordFilter(list(model_keywords_df['keyword']), sentiment_df)\n",
    "    dupe_filter.sentenceCountFilter()\n",
    "    dupe_filter.duplicateFilter()\n",
    "\n",
    "    output = dupe_filter.getKeywordDataFrame()[['keyword', 'sentiment']]\n",
    "    output = list(output.itertuples(index=False, name=None))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great features such', 'POSITIVE'),\n",
       " ('footage stable', 'NEGATIVE'),\n",
       " ('panoramic photo', 'POSITIVE'),\n",
       " ('flatter color grade', 'POSITIVE'),\n",
       " ('max hypersmooth', 'POSITIVE'),\n",
       " ('gopro mounts', 'POSITIVE'),\n",
       " ('record button', 'NEGATIVE'),\n",
       " ('battery performance', 'POSITIVE'),\n",
       " ('time warp feature', 'POSITIVE'),\n",
       " ('button layout', 'POSITIVE'),\n",
       " ('good footage', 'POSITIVE')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GoPro example\n",
    "text = \"(mellow piano music) (upbeat music) - This is the GoPro MAX. And it's GoPro's second attempt at a consumer level, 360 camera. (camera whirring) Last year, Insta360 came out with the One X, and it totally revolutionized the idea of what a 360 camera could be used for. First off, footage shot with the One X was stitched in camera, and then you were able to edit it right on your phone and upload to social platforms almost instantly. Secondly, they market it not as a 360-degree camera but instead a camera that could shoot 360 degrees of possible angles. Now fast forward a year and we have the GoPro Max, which is the successor to the GoPro Fusion that came out in 2017, that was filled with all of the headaches of 360's past. It required two memory cards. You had to stitch the footage on a desktop. But they overhauled all of that with the Max, and they in turn made a really accessible 360 camera, which makes me believe I might continue to actually use this thing. ♪ We wag, we wag, we wag, we wag, we wag ♪ ♪ This is the widest angle GoPro's made ♪ ♪ This is the wide ♪ The GoPro MAX has dual 180-degree lenses that shoot 16.6-megapixel, 360-degree photos, 5.5-megapixel single lens, or GoPro is calling Max SuperView photos, and the super cool 6.2-megapixel panoramic photos called PowerPanos. Video-wise, you have 1440p60 video with a single lens and 5.6K30 spherical video, just like the Insta360. There are mics on all but one side and it shows the same super satisfying rubber buttons and button layout as the Hero8. On the bottom, you have these new stowable mounting prongs, which alas, you no longer need that housing, which I somehow always manage to forget anyways, and it also makes it compatible with all GoPro mounts. The touchscreen on the Max is a bit smaller than the touchscreen on the back of the Hero, but its menu system is identical. The big plus here is if you're vlogging or taking a selfie, you can see yourself much like on the DJI Osmo Action because, well, there's lenses on both sides. The Max is waterproof up to 16 feet. But that proofing is merely for protection at this point. GoPro claims that it's really difficult to stitch underwater footage, but they are promising some sort of waterproof housing in the future. The screen is super responsive, but you can't scroll around your frame in 360 mode since the screen has swiping gestures such as swiping down to reach the main menu. You can however switch between lenses but only while not recording. Once you hit that record button, the screen has no functionality other than just being a viewfinder to the lens that you chose before rolling. The image on the screen will however stay level with the horizon no matter which way you turn the camera. And it does so with almost no noticeable lag. If you lean into that warped, super fisheye look that only a 360 camera could give you, the footage is sick. Skin tones are true to life and it's not afraid to overexpose the highlights of it. I prefer this to the flatter color grade of the One X, especially for users who won't be color correcting. And how even if you are color correcting, I find that the Max and the One X have very little latitude in the coloring process. Now, if you're only gonna be showing these photos on social platforms that you're gonna look at on a phone, who cares, but for a camera that costs $500, I hope it's going a little further than that. (laughs). The Max's 360° video is stabilized in software. It bobs up and down as you'd expect from walking and it can be really jittery, especially at night. The stitching is most noticeable at the top and the bottom of the frame. And since most of GoPro's mounts typically have a wider base than the camera itself, there's almost always a bit of artifact from the mount in the footage. Over-all though, once you export the 360-degree footage, the stitch is pretty clean. The audio, however, is probably the best I've seen on a camera this size. The wind reduction can add a lot of compression, and enough wind can sound like, well this, (wind blowing) Guys, when I say it's windy out, it's like actually very windy out. I am about to blow over. This might make some really good footage as I fly away. But when you're not in 25-mile an hour wind, it actually reduces the sound of wind while prioritizing voices. So while hiking with Alex, I love that the GoPro picked up on the crickets and footsteps, while also keeping our voices at the forefront no matter where the camera was in space. ♪ Yeah, and I think it's gonna be a long, long time ♪ And then there are those PowerPano photos. I gotta be honest guys, I had a tough time not showing you these photos before this video dropped. I mean first and foremost, they make taking a panoramic photo a lot easier. And you don't have to stand there and slowly and robotically move your phone across the landscape. I was really surprised at how crisp these photos turned out to. Even on a desktop, they still look really cool. The time warp feature in 360 mode, which is when the Max just records the time lapse, is really cool, but it's a total battery killer. I saw my battery drop 15% off a fresh charge while shooting this time warp. And I let it record for all of 10 minutes. I think it would help if the screen would go off a bit sooner while shooting these to save some battery. Otherwise, the battery performance is on par with the Hero8. Even though it is a different battery. So yes, you will need to buy extra spares if you plan on heavy use. I was able to get through a full day of heavy use using two of these batteries. 360 cameras are only accessible though if you can do something with the footage. So the software has to be sound, and GoPro's app makes it really easy to edit this footage. Max uses the same GoPro app as the Heroes. Now, I'm no stranger to keyframes, but I think even a casual non-video director from the verge.com could edit the Max's 360 footage. And I actually enjoyed editing it on the app. GoPro also has a 360 desktop app called GoPro Player, which mirrors the mobile app experience with added codec options at export. You have H.264, ProRes, HUVC, and as far as 360 editing goes, I was blown away by its ease. Okay so there's this whole other mode on the Max called Hero mode. And it's basically just when the camera's using one lens as opposed to two. It has more mic controls and it even boasts GoPro's most stable video called Max HyperSmooth. And it also has the widest field of view called Max SuperView. But I gotta be honest, while it does sound good and the footage is stable, the fact that it maxes out at 1440p60 is noticeable. Especially in how much smoothing is being done when the subject is close to the camera. I also noticed the camera struggled to keep the footage level with the horizon, choosing to instead focus on keeping the footage stable. So when climbing down this trail, which to be fair, is a slanted surface, the horizon kept tilting further and further from level to almost a dizzying point. GoPro has become a household name. I mean they're specced out, durable, tiny, portable beasts of cameras. And the Max is no different. And while the Max has great features such as PowerPano, it also has great hardware and a really good touch screen. But for me, it came down to the ease of use of the app that made me wanna keep using this thing. 360 footage used to mean high-end equipment and tons of headaches and post, and that always pushed me away from using it. But when you put all of that into the form factor of a GoPro, a workflow I've been using for years, it's way more approachable. Now, is all of that enough to make this thing more than a novelty camera that will ultimately end up in my forgotten type bed in my basement? Time will truly tell. But it is the most fun and user-friendly 360 camera I have used today. And I'm excited to see this thing as specced out. (coughing) 4K Hero mode? Please? All right, so for 499, you can pre-order a Max today. They start shipping on October 24th and will be on shelves October 27th. The 360 space is getting kinda nuts\"\n",
    "test = outputKeywords(text)\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
